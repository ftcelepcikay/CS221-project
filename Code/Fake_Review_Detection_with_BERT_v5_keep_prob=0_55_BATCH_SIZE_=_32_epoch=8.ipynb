{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fake_Review_Detection_with_BERT_v5_keep_prob=0_55_BATCH_SIZE_=_32_epoch=8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ftcelepcikay/CS221-project/blob/main/Code/Fake_Review_Detection_with_BERT_v5_keep_prob%3D0_55_BATCH_SIZE_%3D_32_epoch%3D8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0a4mTk9o1Qg"
      },
      "source": [
        "# Copyright 2019 Google Inc.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCpvgG0vwXAZ"
      },
      "source": [
        "#Fake Review Detection with BERT on TF Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYrZKaHwV81"
      },
      "source": [
        "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
        "\n",
        "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
        "\n",
        "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XusjFBu8Oqg",
        "outputId": "b60d51dd-36c1-46be-bcbf-7a8c3c2dcfab"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "# import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5wfXDx5SPH"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jviywGyWyKsA",
        "outputId": "f10f5988-fc05-4d67-f123-337a2839ec63"
      },
      "source": [
        "!pip install bert-tensorflow==1.0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow==1.0.1\n",
            "  Downloading bert_tensorflow-1.0.1-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▉                           | 10 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow==1.0.1) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhbGEfwgdEtw",
        "outputId": "a89894e5-f99e-40d0-b82d-69478dbbcc36"
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVB3eOcjxxm1"
      },
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
        "\n",
        "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US_EAnICvP7f",
        "outputId": "c38225b8-2c7d-409a-e1e8-3f287d7a0c41"
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'OUTPUT_DIR_NAME5'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = False #@param {type:\"boolean\"}\n",
        "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "USE_BUCKET = False #@param {type:\"boolean\"}\n",
        "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Model output directory: OUTPUT_DIR_NAME5 *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lur1qPzeQpt6",
        "outputId": "2caa3722-ca29-4add-98e9-ebe7f6a74cad"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJhFn_Ht9srZ",
        "outputId": "e72e0852-32fb-4de1-e01f-c7078bf8b46d"
      },
      "source": [
        "!ls /content/gdrive/MyDrive/CS221/Dataset/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "testing_matrix.csv   training_matrix_ext.csv\n",
            "training_matrix.csv  validating_matrix.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PQVRdULRmfg"
      },
      "source": [
        "train = pd.read_csv(r'/content/gdrive/MyDrive/CS221/Dataset/training_matrix.csv')\n",
        "valid = pd.read_csv(r'/content/gdrive/MyDrive/CS221/Dataset/validating_matrix.csv')\n",
        "test = pd.read_csv(r'/content/gdrive/MyDrive/CS221/Dataset/testing_matrix.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMuzjhM0d_mj",
        "outputId": "22e21f8d-5b5d-43c7-99db-216a9d93e2c1"
      },
      "source": [
        "train.head"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of              date  user_id  ...  title_word_count upper_case_word_count\n",
              "0      2011-08-11    25002  ...                 3                     0\n",
              "1      2011-03-22     7756  ...                11                     6\n",
              "2      2011-02-11   139209  ...                13                     4\n",
              "3      2011-07-30     2400  ...                13                     2\n",
              "4      2011-05-23    32189  ...                10                     4\n",
              "...           ...      ...  ...               ...                   ...\n",
              "11119  2011-12-31    49996  ...                 6                     2\n",
              "11120  2011-12-31    93088  ...                 2                     1\n",
              "11121  2011-12-31    89621  ...                 4                     2\n",
              "11122  2011-12-31   117284  ...                 2                     1\n",
              "11123  2011-12-31    46249  ...                29                     3\n",
              "\n",
              "[11124 rows x 13 columns]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7treDq-eYea"
      },
      "source": [
        "train.loc[train.label == 1, 'label'] = 0\n",
        "train.loc[train.label == -1, 'label'] = 1\n",
        "\n",
        "valid.loc[valid.label == 1, 'label'] = 0\n",
        "valid.loc[valid.label == -1, 'label'] = 1\n",
        "\n",
        "test.loc[test.label == 1, 'label'] = 0\n",
        "test.loc[test.label == -1, 'label'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj_2bmWqE9NL"
      },
      "source": [
        "import string\n",
        "train['char_count'] = train['review'].apply(len)\n",
        "train['word_count'] = train['review'].apply(lambda x: len(x.split()))\n",
        "train['word_density'] = train['char_count'] / (train['word_count']+1)\n",
        "train['punctuation_count'] = train['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "train['title_word_count'] = train['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "train['upper_case_word_count'] = train['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
        "\n",
        "valid['char_count'] = valid['review'].apply(len)\n",
        "valid['word_count'] = valid['review'].apply(lambda x: len(x.split()))\n",
        "valid['word_density'] = valid['char_count'] / (valid['word_count']+1)\n",
        "valid['punctuation_count'] = valid['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "valid['title_word_count'] = valid['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "valid['upper_case_word_count'] = valid['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
        "\n",
        "test['char_count'] = test['review'].apply(len)\n",
        "test['word_count'] = test['review'].apply(lambda x: len(x.split()))\n",
        "test['word_density'] = test['char_count'] / (test['word_count']+1)\n",
        "test['punctuation_count'] = test['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "test['title_word_count'] = test['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "test['upper_case_word_count'] = test['review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQChnkIRFDRO",
        "outputId": "771a6a83-a156-4c9b-aa54-cba1d52a1f1b"
      },
      "source": [
        "# reviewer_centric features #\n",
        "#train.user_id.value_counts()\n",
        "\n",
        "train['date.1'] = pd.to_datetime(train['date.1'])\n",
        "train['day_of_week'] = train['date.1'].dt.day_name()\n",
        "#train.day_of_week.value_counts()[:10]]\n",
        "dofw = train.day_of_week.unique()\n",
        "for day in range(len(dofw)):\n",
        "        label=train['label'][train.day_of_week == dofw[day]]\n",
        "        fake=len(label[label==1])\n",
        "        real=len(label[label==0])\n",
        "        \n",
        "        print(\"***********Day is \",dofw[day])\n",
        "        print(\"fake is \",fake)\n",
        "        print(\"real is \",real)\n",
        "        print(\"tot is \",fake+real)\n",
        "        print(\"Ratio is \",100*(fake/(fake+real)))\n",
        "        print(\"len is \",len(label))\n",
        "\n",
        "train['user_id_no_of_review'] = train.groupby('user_id')['user_id'].transform('size')\n",
        "train['user_id_ave_rating'] = train.groupby('user_id')['rating'].transform('mean')\n",
        "#train['user_id_std_rating'] = train.groupby('user_id')['rating'].transform('std')\n",
        "train['user_id_ave_no_words'] = train.groupby('user_id')['word_count'].transform('mean')\n",
        "train['user_id_max_review_a_day'] = train['user_id_no_of_review']\n",
        "grouped = train.groupby('user_id')\n",
        "\n",
        "for name,group in grouped:\n",
        "    #print(name)\n",
        "    #print(group)\n",
        "    df2 = group.groupby('date').size().max()\n",
        "    #print(df2)\n",
        "    train.loc[train.user_id == name,'user_id_max_review_a_day'] = df2\n",
        "    #group['user_id_max_review_a_day']=df2\n",
        "\n",
        "#train[:][train.user_id==9236].sort_index(axis = 0)\n",
        "    \n",
        "valid['date.1'] = pd.to_datetime(valid['date.1'])\n",
        "valid['day_of_week'] = valid['date.1'].dt.day_name()\n",
        "#valid.day_of_week.value_counts()[:10]]\n",
        "dofw = valid.day_of_week.unique()\n",
        "for day in range(len(dofw)):\n",
        "        label=valid['label'][valid.day_of_week == dofw[day]]\n",
        "        fake=len(label[label==1])\n",
        "        real=len(label[label==0])\n",
        "        \n",
        "        print(\"***********Day is \",dofw[day])\n",
        "        print(\"fake is \",fake)\n",
        "        print(\"real is \",real)\n",
        "        print(\"tot is \",fake+real)\n",
        "        print(\"Ratio is \",100*(fake/(fake+real)))\n",
        "        print(\"len is \",len(label))\n",
        "\n",
        "valid['user_id_no_of_review'] = valid.groupby('user_id')['user_id'].transform('size')\n",
        "valid['user_id_ave_rating'] = valid.groupby('user_id')['rating'].transform('mean')\n",
        "#valid['user_id_std_rating'] = valid.groupby('user_id')['rating'].transform('std')\n",
        "valid['user_id_ave_no_words'] = valid.groupby('user_id')['word_count'].transform('mean')\n",
        "valid['user_id_max_review_a_day'] = valid['user_id_no_of_review']\n",
        "grouped = valid.groupby('user_id')\n",
        "\n",
        "for name,group in grouped:\n",
        "    #print(name)\n",
        "    #print(group)\n",
        "    df2 = group.groupby('date').size().max()\n",
        "    #print(df2)\n",
        "    valid.loc[valid.user_id == name,'user_id_max_review_a_day'] = df2\n",
        "    #group['user_id_max_review_a_day']=df2\n",
        "\n",
        "#valid[:][valid.user_id==9236].sort_index(axis = 0) \n",
        "\n",
        "test['date.1'] = pd.to_datetime(test['date.1'])\n",
        "test['day_of_week'] = test['date.1'].dt.day_name()\n",
        "#test.day_of_week.value_counts()[:10]]\n",
        "dofw = test.day_of_week.unique()\n",
        "for day in range(len(dofw)):\n",
        "        label=test['label'][test.day_of_week == dofw[day]]\n",
        "        fake=len(label[label==1])\n",
        "        real=len(label[label==0])\n",
        "        \n",
        "        print(\"***********Day is \",dofw[day])\n",
        "        print(\"fake is \",fake)\n",
        "        print(\"real is \",real)\n",
        "        print(\"tot is \",fake+real)\n",
        "        print(\"Ratio is \",100*(fake/(fake+real)))\n",
        "        print(\"len is \",len(label))\n",
        "\n",
        "test['user_id_no_of_review'] = test.groupby('user_id')['user_id'].transform('size')\n",
        "test['user_id_ave_rating'] = test.groupby('user_id')['rating'].transform('mean')\n",
        "#test['user_id_std_rating'] = test.groupby('user_id')['rating'].transform('std')\n",
        "test['user_id_ave_no_words'] = test.groupby('user_id')['word_count'].transform('mean')\n",
        "test['user_id_max_review_a_day'] = test['user_id_no_of_review']\n",
        "grouped = test.groupby('user_id')\n",
        "\n",
        "for name,group in grouped:\n",
        "    #print(name)\n",
        "    #print(group)\n",
        "    df2 = group.groupby('date').size().max()\n",
        "    #print(df2)\n",
        "    test.loc[test.user_id == name,'user_id_max_review_a_day'] = df2\n",
        "    #group['user_id_max_review_a_day']=df2\n",
        "\n",
        "#test[:][test.user_id==9236].sort_index(axis = 0) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***********Day is  Thursday\n",
            "fake is  764\n",
            "real is  755\n",
            "tot is  1519\n",
            "Ratio is  50.29624753127058\n",
            "len is  1519\n",
            "***********Day is  Tuesday\n",
            "fake is  800\n",
            "real is  788\n",
            "tot is  1588\n",
            "Ratio is  50.377833753148614\n",
            "len is  1588\n",
            "***********Day is  Friday\n",
            "fake is  758\n",
            "real is  701\n",
            "tot is  1459\n",
            "Ratio is  51.953392734749826\n",
            "len is  1459\n",
            "***********Day is  Saturday\n",
            "fake is  756\n",
            "real is  744\n",
            "tot is  1500\n",
            "Ratio is  50.4\n",
            "len is  1500\n",
            "***********Day is  Monday\n",
            "fake is  858\n",
            "real is  913\n",
            "tot is  1771\n",
            "Ratio is  48.4472049689441\n",
            "len is  1771\n",
            "***********Day is  Sunday\n",
            "fake is  790\n",
            "real is  903\n",
            "tot is  1693\n",
            "Ratio is  46.66272888363851\n",
            "len is  1693\n",
            "***********Day is  Wednesday\n",
            "fake is  836\n",
            "real is  758\n",
            "tot is  1594\n",
            "Ratio is  52.446675031367626\n",
            "len is  1594\n",
            "***********Day is  Tuesday\n",
            "fake is  150\n",
            "real is  161\n",
            "tot is  311\n",
            "Ratio is  48.231511254019296\n",
            "len is  311\n",
            "***********Day is  Thursday\n",
            "fake is  129\n",
            "real is  128\n",
            "tot is  257\n",
            "Ratio is  50.19455252918288\n",
            "len is  257\n",
            "***********Day is  Saturday\n",
            "fake is  137\n",
            "real is  117\n",
            "tot is  254\n",
            "Ratio is  53.937007874015755\n",
            "len is  254\n",
            "***********Day is  Monday\n",
            "fake is  169\n",
            "real is  166\n",
            "tot is  335\n",
            "Ratio is  50.44776119402985\n",
            "len is  335\n",
            "***********Day is  Wednesday\n",
            "fake is  159\n",
            "real is  141\n",
            "tot is  300\n",
            "Ratio is  53.0\n",
            "len is  300\n",
            "***********Day is  Sunday\n",
            "fake is  138\n",
            "real is  153\n",
            "tot is  291\n",
            "Ratio is  47.42268041237113\n",
            "len is  291\n",
            "***********Day is  Friday\n",
            "fake is  118\n",
            "real is  134\n",
            "tot is  252\n",
            "Ratio is  46.82539682539682\n",
            "len is  252\n",
            "***********Day is  Friday\n",
            "fake is  661\n",
            "real is  638\n",
            "tot is  1299\n",
            "Ratio is  50.88529638183218\n",
            "len is  1299\n",
            "***********Day is  Wednesday\n",
            "fake is  728\n",
            "real is  637\n",
            "tot is  1365\n",
            "Ratio is  53.333333333333336\n",
            "len is  1365\n",
            "***********Day is  Tuesday\n",
            "fake is  754\n",
            "real is  734\n",
            "tot is  1488\n",
            "Ratio is  50.67204301075269\n",
            "len is  1488\n",
            "***********Day is  Sunday\n",
            "fake is  704\n",
            "real is  807\n",
            "tot is  1511\n",
            "Ratio is  46.591661151555265\n",
            "len is  1511\n",
            "***********Day is  Saturday\n",
            "fake is  638\n",
            "real is  660\n",
            "tot is  1298\n",
            "Ratio is  49.152542372881356\n",
            "len is  1298\n",
            "***********Day is  Thursday\n",
            "fake is  681\n",
            "real is  645\n",
            "tot is  1326\n",
            "Ratio is  51.35746606334841\n",
            "len is  1326\n",
            "***********Day is  Monday\n",
            "fake is  834\n",
            "real is  879\n",
            "tot is  1713\n",
            "Ratio is  48.68651488616462\n",
            "len is  1713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psganAbBFHbC"
      },
      "source": [
        "# product_centric features #\n",
        "#train.prod_id.value_counts()\n",
        "\n",
        "train['prod_id_no_of_review'] = train.groupby('prod_id')['prod_id'].transform('size') + 1000.\n",
        "train['prod_id_ave_rating'] = train.groupby('prod_id')['rating'].transform('mean') + 1000.\n",
        "#train['prod_id_std_rating'] = train.groupby('prod_id')['rating'].transform('std') + 1000.\n",
        "train['prod_id_ave_no_words'] = train.groupby('prod_id')['word_count'].transform('mean') + 1000.\n",
        "train['prod_id_max_review_a_day'] = train['prod_id_no_of_review'] + 1000.\n",
        "grouped = train.groupby('prod_id')\n",
        "\n",
        "for name,group in grouped:\n",
        "    #print(name)\n",
        "    #print(group)\n",
        "    df2 = group.groupby('date').size().max()\n",
        "    #print(df2)\n",
        "    train.loc[train.prod_id == name,'prod_id_max_review_a_day'] = df2 + 1000.\n",
        "    #group['prod_id_max_review_a_day']=df2\n",
        "\n",
        "#train[:][train.prod_id==9236].sort_index(axis = 0)\n",
        "\n",
        "valid['prod_id_no_of_review'] = valid.groupby('prod_id')['prod_id'].transform('size') + 1000.\n",
        "valid['prod_id_ave_rating'] = valid.groupby('prod_id')['rating'].transform('mean') + 1000.\n",
        "#valid['prod_id_std_rating'] = valid.groupby('prod_id')['rating'].transform('std') + 1000.\n",
        "valid['prod_id_ave_no_words'] = valid.groupby('prod_id')['word_count'].transform('mean') + 1000.\n",
        "valid['prod_id_max_review_a_day'] = valid['prod_id_no_of_review']\n",
        "grouped = valid.groupby('prod_id')\n",
        "\n",
        "for name,group in grouped:\n",
        "    #print(name)\n",
        "    #print(group)\n",
        "    df2 = group.groupby('date').size().max()\n",
        "    #print(df2)\n",
        "    valid.loc[valid.prod_id == name,'prod_id_max_review_a_day'] = df2 + 1000.\n",
        "    #group['prod_id_max_review_a_day']=df2\n",
        "\n",
        "#valid[:][valid.prod_id==9236].sort_index(axis = 0)  \n",
        "\n",
        "test['prod_id_no_of_review'] = test.groupby('prod_id')['prod_id'].transform('size') + 1000.\n",
        "test['prod_id_ave_rating'] = test.groupby('prod_id')['rating'].transform('mean') + 1000.\n",
        "#test['prod_id_std_rating'] = test.groupby('prod_id')['rating'].transform('std') + 1000.\n",
        "test['prod_id_ave_no_words'] = test.groupby('prod_id')['word_count'].transform('mean') + 1000.\n",
        "test['prod_id_max_review_a_day'] = test['prod_id_no_of_review']\n",
        "grouped = test.groupby('prod_id')\n",
        "\n",
        "for name,group in grouped:\n",
        "    #print(name)\n",
        "    #print(group)\n",
        "    df2 = group.groupby('date').size().max()\n",
        "    #print(df2)\n",
        "    test.loc[test.prod_id == name,'prod_id_max_review_a_day'] = df2 + 1000.\n",
        "    #group['prod_id_max_review_a_day']=df2\n",
        "\n",
        "#test[:][test.prod_id==9236].sort_index(axis = 0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2M0RfyGF_U6"
      },
      "source": [
        "train['total']=train.user_id.astype(str) + ' '+ train.prod_id.astype(str) + ' ' + train.rating.astype(str) + ' '+ \\\n",
        "train.user_id_no_of_review.astype(str) + ' ' + train.user_id_ave_rating.astype(str) + ' '+\\\n",
        "train.user_id_ave_no_words.astype(str) + ' ' + train.user_id_max_review_a_day.astype(str) + ' '+\\\n",
        "train.prod_id_no_of_review.astype(str) + ' ' + train.prod_id_ave_rating.astype(str) + ' '+\\\n",
        "train.prod_id_ave_no_words.astype(str) + ' ' + train.prod_id_max_review_a_day.astype(str) + ' '+\\\n",
        "train.day_of_week.astype(str) + ' '+train.review  \n",
        "\n",
        "#train['total']=train.user_id.astype(str) + ' '+ train.prod_id.astype(str) + ' ' + train.rating.astype(str) + ' '+  train.review\n",
        "#test['total']=test.user_id.astype(str) + ' '+ test.prod_id.astype(str) + ' ' + test.rating.astype(str) + ' '+test.review\n",
        "\n",
        "valid['total']=valid.user_id.astype(str) + ' '+ valid.prod_id.astype(str) + ' ' + valid.rating.astype(str) + ' '+ \\\n",
        "valid.user_id_no_of_review.astype(str) + ' ' + valid.user_id_ave_rating.astype(str) + ' '+\\\n",
        "valid.user_id_ave_no_words.astype(str) + ' ' + valid.user_id_max_review_a_day.astype(str) + ' '+\\\n",
        "valid.prod_id_no_of_review.astype(str) + ' ' + valid.prod_id_ave_rating.astype(str) + ' '+\\\n",
        "valid.prod_id_ave_no_words.astype(str) + ' ' + valid.prod_id_max_review_a_day.astype(str) + ' '+\\\n",
        "valid.day_of_week.astype(str) + ' '+ valid.review  \n",
        "\n",
        "test['total']=test.user_id.astype(str) + ' '+ test.prod_id.astype(str) + ' ' + test.rating.astype(str) + ' '+ \\\n",
        "test.user_id_no_of_review.astype(str) + ' ' + test.user_id_ave_rating.astype(str) + ' '+\\\n",
        "test.user_id_ave_no_words.astype(str) + ' ' + test.user_id_max_review_a_day.astype(str) + ' '+\\\n",
        "test.prod_id_no_of_review.astype(str) + ' ' + test.prod_id_ave_rating.astype(str) + ' '+\\\n",
        "test.prod_id_ave_no_words.astype(str) + ' ' + test.prod_id_max_review_a_day.astype(str) + ' '+\\\n",
        "test.day_of_week.astype(str) + ' '+ test.review  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_w8SRqN0fr"
      },
      "source": [
        "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ydRqUPz9Y4Y",
        "outputId": "6f093961-4371-4cf0-ae01-46805ab58bb3"
      },
      "source": [
        "test.total.head"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of 0       102359 514 5.0 1 5.0 7.0 1 1013.0 1003.4615384...\n",
              "1       45961 763 5.0 1 5.0 76.0 1 1002.0 1005.0 1044....\n",
              "2       78578 379 5.0 1 5.0 211.0 1 1021.0 1004.333333...\n",
              "3       85624 419 5.0 1 5.0 19.0 1 1007.0 1003.7142857...\n",
              "4       33686 101 4.0 1 4.0 14.0 1 1032.0 1004.375 107...\n",
              "                              ...                        \n",
              "9995    21818 841 3.0 1 3.0 513.0 1 1058.0 1003.396551...\n",
              "9996    7790 706 4.0 1 4.0 300.0 1 1017.0 1003.8235294...\n",
              "9997    101508 540 4.0 1 4.0 51.0 1 1009.0 1004.111111...\n",
              "9998    8119 914 4.0 1 4.0 45.0 1 1015.0 1003.8 1116.3...\n",
              "9999    3521 453 3.0 2 3.5 95.5 1 1025.0 1003.68 1157....\n",
              "Name: total, Length: 10000, dtype: object>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8WHJgzhIZf"
      },
      "source": [
        "To keep train fast, we'll take a sample of 5000 train and test examples, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prRQM8pDi8xI",
        "outputId": "54d9a1d6-aecb-4185-c656-f7e0fd31b65f"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['date', 'user_id', 'prod_id', 'date.1', 'review', 'label', 'rating',\n",
              "       'char_count', 'word_count', 'word_density', 'punctuation_count',\n",
              "       'title_word_count', 'upper_case_word_count', 'day_of_week',\n",
              "       'user_id_no_of_review', 'user_id_ave_rating', 'user_id_ave_no_words',\n",
              "       'user_id_max_review_a_day', 'prod_id_no_of_review',\n",
              "       'prod_id_ave_rating', 'prod_id_ave_no_words',\n",
              "       'prod_id_max_review_a_day', 'total'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlMDj4a4NtYf"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "train = shuffle(train, random_state=0)\n",
        "valid = shuffle(valid, random_state=0)\n",
        "test = shuffle(test, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRnHSz3iSXz"
      },
      "source": [
        "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it"
      },
      "source": [
        "DATA_COLUMN = 'total'\n",
        "LABEL_COLUMN = 'label'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V399W0rqNJ-Z"
      },
      "source": [
        "#Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gEt5SmM6i6"
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "valid_InputExamples = valid.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZWZtKxObjh"
      },
      "source": [
        "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Happily, we don't have to worry about most of these details.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWiDtpyQSoU"
      },
      "source": [
        "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhJSe0QHNG7U",
        "outputId": "f9e02b85-4e19-4ae2-bae1-a2d1b3d439db"
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4oFkhpZBDKm"
      },
      "source": [
        "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsBo6RCtQmwx",
        "outputId": "a3aeb350-088e-4da7-f7bf-35870ef7a41b"
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OEzfFIt6GIc"
      },
      "source": [
        "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL5W8gEGRTAf",
        "outputId": "85f8a7b9-46f3-4519-939d-dd4b5ace2ed6"
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "valid_features = bert.run_classifier.convert_examples_to_features(valid_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 11124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 11124\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 42 ##7 ##75 81 ##4 4 . 0 5 2 . 8 289 . 0 2 102 ##3 . 0 100 ##4 . 43 ##47 ##8 ##26 ##0 ##86 ##9 ##56 109 ##5 . 130 ##43 ##47 ##8 ##26 ##0 ##8 ##7 100 ##1 . 0 thursday i have been meaning to write a review for a while now because i was so happy with the meal i ate here the day after valentine ##s day . my girlfriend and i went , we both wait tables and decided to make money on v - day then spend it the night after . the atmosphere was awesome , i love the lodge ##y rustic feel . the waits ##taff was informed but not pre ##ten ##tious [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 42 ##7 ##75 81 ##4 4 . 0 5 2 . 8 289 . 0 2 102 ##3 . 0 100 ##4 . 43 ##47 ##8 ##26 ##0 ##86 ##9 ##56 109 ##5 . 130 ##43 ##47 ##8 ##26 ##0 ##8 ##7 100 ##1 . 0 thursday i have been meaning to write a review for a while now because i was so happy with the meal i ate here the day after valentine ##s day . my girlfriend and i went , we both wait tables and decided to make money on v - day then spend it the night after . the atmosphere was awesome , i love the lodge ##y rustic feel . the waits ##taff was informed but not pre ##ten ##tious [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4413 2581 23352 6282 2549 1018 1012 1014 1019 1016 1012 1022 27054 1012 1014 1016 9402 2509 1012 1014 2531 2549 1012 4724 22610 2620 23833 2692 20842 2683 26976 11518 2629 1012 7558 23777 22610 2620 23833 2692 2620 2581 2531 2487 1012 1014 9432 1045 2031 2042 3574 2000 4339 1037 3319 2005 1037 2096 2085 2138 1045 2001 2061 3407 2007 1996 7954 1045 8823 2182 1996 2154 2044 10113 2015 2154 1012 2026 6513 1998 1045 2253 1010 2057 2119 3524 7251 1998 2787 2000 2191 2769 2006 1058 1011 2154 2059 5247 2009 1996 2305 2044 1012 1996 7224 2001 12476 1010 1045 2293 1996 7410 2100 27471 2514 1012 1996 18074 22542 2001 6727 2021 2025 3653 6528 20771 102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4413 2581 23352 6282 2549 1018 1012 1014 1019 1016 1012 1022 27054 1012 1014 1016 9402 2509 1012 1014 2531 2549 1012 4724 22610 2620 23833 2692 20842 2683 26976 11518 2629 1012 7558 23777 22610 2620 23833 2692 2620 2581 2531 2487 1012 1014 9432 1045 2031 2042 3574 2000 4339 1037 3319 2005 1037 2096 2085 2138 1045 2001 2061 3407 2007 1996 7954 1045 8823 2182 1996 2154 2044 10113 2015 2154 1012 2026 6513 1998 1045 2253 1010 2057 2119 3524 7251 1998 2787 2000 2191 2769 2006 1058 1011 2154 2059 5247 2009 1996 2305 2044 1012 1996 7224 2001 12476 1010 1045 2293 1996 7410 2100 27471 2514 1012 1996 18074 22542 2001 6727 2021 2025 3653 6528 20771 102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 39 ##6 ##8 ##4 211 2 . 0 2 3 . 5 23 . 0 1 109 ##9 . 0 100 ##3 . 76 ##7 ##6 ##7 ##6 ##7 ##6 ##7 ##6 ##7 ##6 ##8 107 ##8 . 49 ##49 ##49 ##49 ##49 ##49 ##4 100 ##3 . 0 wednesday yu ##ck ! ! i waited in line along with a troop of un ##bat ##hed hips ##ters only to have the gr ##ea ##sies ##t pizza coming out of the b rated health hazard . . . . 2 stars for simply being a pizza spot . [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 39 ##6 ##8 ##4 211 2 . 0 2 3 . 5 23 . 0 1 109 ##9 . 0 100 ##3 . 76 ##7 ##6 ##7 ##6 ##7 ##6 ##7 ##6 ##7 ##6 ##8 107 ##8 . 49 ##49 ##49 ##49 ##49 ##49 ##4 100 ##3 . 0 wednesday yu ##ck ! ! i waited in line along with a troop of un ##bat ##hed hips ##ters only to have the gr ##ea ##sies ##t pizza coming out of the b rated health hazard . . . . 2 stars for simply being a pizza spot . [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4464 2575 2620 2549 19235 1016 1012 1014 1016 1017 1012 1019 2603 1012 1014 1015 11518 2683 1012 1014 2531 2509 1012 6146 2581 2575 2581 2575 2581 2575 2581 2575 2581 2575 2620 10550 2620 1012 4749 26224 26224 26224 26224 26224 2549 2531 2509 1012 1014 9317 9805 3600 999 999 1045 4741 1999 2240 2247 2007 1037 10123 1997 4895 14479 9072 6700 7747 2069 2000 2031 1996 24665 5243 14625 2102 10733 2746 2041 1997 1996 1038 6758 2740 15559 1012 1012 1012 1012 1016 3340 2005 3432 2108 1037 10733 3962 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4464 2575 2620 2549 19235 1016 1012 1014 1016 1017 1012 1019 2603 1012 1014 1015 11518 2683 1012 1014 2531 2509 1012 6146 2581 2575 2581 2575 2581 2575 2581 2575 2581 2575 2620 10550 2620 1012 4749 26224 26224 26224 26224 26224 2549 2531 2509 1012 1014 9317 9805 3600 999 999 1045 4741 1999 2240 2247 2007 1037 10123 1997 4895 14479 9072 6700 7747 2069 2000 2031 1996 24665 5243 14625 2102 10733 2746 2041 1997 1996 1038 6758 2740 15559 1012 1012 1012 1012 1016 3340 2005 3432 2108 1037 10733 3962 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 1862 ##9 590 4 . 0 1 4 . 0 187 . 0 1 101 ##9 . 0 100 ##4 . 05 ##26 ##31 ##57 ##8 ##9 ##47 ##4 110 ##5 . 89 ##47 ##36 ##8 ##42 ##10 ##52 100 ##1 . 0 friday after failing to elbow our way through the insane crowds at the halloween parade , my friend and i decided to retreat to a nice and qu ##aint restaurant for some proper food . this small and cozy establishment definitely made up for our disappointing halloween parade experience ! i had : atlantic cod - a french - japanese fusion fish dish containing supreme - quality shit ##ake mushrooms bas ##king in an equally supreme - tasting mushroom bro ##th my friend [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 1862 ##9 590 4 . 0 1 4 . 0 187 . 0 1 101 ##9 . 0 100 ##4 . 05 ##26 ##31 ##57 ##8 ##9 ##47 ##4 110 ##5 . 89 ##47 ##36 ##8 ##42 ##10 ##52 100 ##1 . 0 friday after failing to elbow our way through the insane crowds at the halloween parade , my friend and i decided to retreat to a nice and qu ##aint restaurant for some proper food . this small and cozy establishment definitely made up for our disappointing halloween parade experience ! i had : atlantic cod - a french - japanese fusion fish dish containing supreme - quality shit ##ake mushrooms bas ##king in an equally supreme - tasting mushroom bro ##th my friend [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6889 2683 25186 1018 1012 1014 1015 1018 1012 1014 19446 1012 1014 1015 7886 2683 1012 1014 2531 2549 1012 5709 23833 21486 28311 2620 2683 22610 2549 7287 2629 1012 6486 22610 21619 2620 20958 10790 25746 2531 2487 1012 1014 5958 2044 7989 2000 8999 2256 2126 2083 1996 9577 12783 2012 1996 14414 7700 1010 2026 2767 1998 1045 2787 2000 7822 2000 1037 3835 1998 24209 22325 4825 2005 2070 5372 2833 1012 2023 2235 1998 26931 5069 5791 2081 2039 2005 2256 15640 14414 7700 3325 999 1045 2018 1024 4448 19429 1011 1037 2413 1011 2887 10077 3869 9841 4820 4259 1011 3737 4485 13808 23827 19021 6834 1999 2019 8053 4259 1011 18767 18565 22953 2705 2026 2767 102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6889 2683 25186 1018 1012 1014 1015 1018 1012 1014 19446 1012 1014 1015 7886 2683 1012 1014 2531 2549 1012 5709 23833 21486 28311 2620 2683 22610 2549 7287 2629 1012 6486 22610 21619 2620 20958 10790 25746 2531 2487 1012 1014 5958 2044 7989 2000 8999 2256 2126 2083 1996 9577 12783 2012 1996 14414 7700 1010 2026 2767 1998 1045 2787 2000 7822 2000 1037 3835 1998 24209 22325 4825 2005 2070 5372 2833 1012 2023 2235 1998 26931 5069 5791 2081 2039 2005 2256 15640 14414 7700 3325 999 1045 2018 1024 4448 19429 1011 1037 2413 1011 2887 10077 3869 9841 4820 4259 1011 3737 4485 13808 23827 19021 6834 1999 2019 8053 4259 1011 18767 18565 22953 2705 2026 2767 102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 75 ##18 319 4 . 0 1 4 . 0 78 . 0 1 102 ##2 . 0 100 ##4 . 04 ##54 ##54 ##54 ##54 ##54 ##5 112 ##4 . 227 ##27 ##27 ##27 ##27 ##27 100 ##1 . 0 tuesday i loved this place ! went here with my dinner club crew and had a great time . this place is super cute and cozy - decor is on point with very helpful and friendly staff . the food was delicious and very reasonably priced ! we went on a week night around 7 ##pm and had the place basically to ourselves , which was nice so our laughter didn ' t disturb anyone else . this place is definitely on my list of [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 75 ##18 319 4 . 0 1 4 . 0 78 . 0 1 102 ##2 . 0 100 ##4 . 04 ##54 ##54 ##54 ##54 ##54 ##5 112 ##4 . 227 ##27 ##27 ##27 ##27 ##27 100 ##1 . 0 tuesday i loved this place ! went here with my dinner club crew and had a great time . this place is super cute and cozy - decor is on point with very helpful and friendly staff . the food was delicious and very reasonably priced ! we went on a week night around 7 ##pm and had the place basically to ourselves , which was nice so our laughter didn ' t disturb anyone else . this place is definitely on my list of [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4293 15136 26499 1018 1012 1014 1015 1018 1012 1014 6275 1012 1014 1015 9402 2475 1012 1014 2531 2549 1012 5840 27009 27009 27009 27009 27009 2629 11176 2549 1012 21489 22907 22907 22907 22907 22907 2531 2487 1012 1014 9857 1045 3866 2023 2173 999 2253 2182 2007 2026 4596 2252 3626 1998 2018 1037 2307 2051 1012 2023 2173 2003 3565 10140 1998 26931 1011 25545 2003 2006 2391 2007 2200 14044 1998 5379 3095 1012 1996 2833 2001 12090 1998 2200 16286 21125 999 2057 2253 2006 1037 2733 2305 2105 1021 9737 1998 2018 1996 2173 10468 2000 9731 1010 2029 2001 3835 2061 2256 7239 2134 1005 1056 22995 3087 2842 1012 2023 2173 2003 5791 2006 2026 2862 1997 102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4293 15136 26499 1018 1012 1014 1015 1018 1012 1014 6275 1012 1014 1015 9402 2475 1012 1014 2531 2549 1012 5840 27009 27009 27009 27009 27009 2629 11176 2549 1012 21489 22907 22907 22907 22907 22907 2531 2487 1012 1014 9857 1045 3866 2023 2173 999 2253 2182 2007 2026 4596 2252 3626 1998 2018 1037 2307 2051 1012 2023 2173 2003 3565 10140 1998 26931 1011 25545 2003 2006 2391 2007 2200 14044 1998 5379 3095 1012 1996 2833 2001 12090 1998 2200 16286 21125 999 2057 2253 2006 1037 2733 2305 2105 1021 9737 1998 2018 1996 2173 10468 2000 9731 1010 2029 2001 3835 2061 2256 7239 2134 1005 1056 22995 3087 2842 1012 2023 2173 2003 5791 2006 2026 2862 1997 102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 45 ##6 ##32 70 ##2 3 . 0 1 3 . 0 74 . 0 1 104 ##6 . 0 100 ##3 . 73 ##9 ##13 ##0 ##43 ##47 ##8 ##26 106 ##5 . 1739 ##13 ##0 ##43 ##47 ##8 ##3 100 ##2 . 0 wednesday this is a good place to order from . in the summer it is great to sit in the back . i love the mango salad . all of the noodles are all good . with to ##fu or chicken . . . the beef is not good ( tough and no flavor ) . joy ##a is well priced and fast to deliver . it think it is pretty standard fair - clean , fresh and ta ##sty . [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 45 ##6 ##32 70 ##2 3 . 0 1 3 . 0 74 . 0 1 104 ##6 . 0 100 ##3 . 73 ##9 ##13 ##0 ##43 ##47 ##8 ##26 106 ##5 . 1739 ##13 ##0 ##43 ##47 ##8 ##3 100 ##2 . 0 wednesday this is a good place to order from . in the summer it is great to sit in the back . i love the mango salad . all of the noodles are all good . with to ##fu or chicken . . . the beef is not good ( tough and no flavor ) . joy ##a is well priced and fast to deliver . it think it is pretty standard fair - clean , fresh and ta ##sty . [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3429 2575 16703 3963 2475 1017 1012 1014 1015 1017 1012 1014 6356 1012 1014 1015 9645 2575 1012 1014 2531 2509 1012 6421 2683 17134 2692 23777 22610 2620 23833 10114 2629 1012 25801 17134 2692 23777 22610 2620 2509 2531 2475 1012 1014 9317 2023 2003 1037 2204 2173 2000 2344 2013 1012 1999 1996 2621 2009 2003 2307 2000 4133 1999 1996 2067 1012 1045 2293 1996 24792 16521 1012 2035 1997 1996 27130 2024 2035 2204 1012 2007 2000 11263 2030 7975 1012 1012 1012 1996 12486 2003 2025 2204 1006 7823 1998 2053 14894 1007 1012 6569 2050 2003 2092 21125 1998 3435 2000 8116 1012 2009 2228 2009 2003 3492 3115 4189 1011 4550 1010 4840 1998 11937 21756 1012 102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3429 2575 16703 3963 2475 1017 1012 1014 1015 1017 1012 1014 6356 1012 1014 1015 9645 2575 1012 1014 2531 2509 1012 6421 2683 17134 2692 23777 22610 2620 23833 10114 2629 1012 25801 17134 2692 23777 22610 2620 2509 2531 2475 1012 1014 9317 2023 2003 1037 2204 2173 2000 2344 2013 1012 1999 1996 2621 2009 2003 2307 2000 4133 1999 1996 2067 1012 1045 2293 1996 24792 16521 1012 2035 1997 1996 27130 2024 2035 2204 1012 2007 2000 11263 2030 7975 1012 1012 1012 1996 12486 2003 2025 2204 1006 7823 1998 2053 14894 1007 1012 6569 2050 2003 2092 21125 1998 3435 2000 8116 1012 2009 2228 2009 2003 3492 3115 4189 1011 4550 1010 4840 1998 11937 21756 1012 102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 10000 of 11124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 10000 of 11124\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 2000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 72 ##48 ##8 276 4 . 0 1 4 . 0 31 . 0 1 100 ##3 . 0 100 ##4 . 333 ##33 ##33 ##33 ##33 ##34 103 ##9 . 0 100 ##1 . 0 sunday had a good time here today with my family . food was very good , wine a bit over ##pr ##ice ##d . only down ##beat was the \" mai ##tre ' d ' \" - he kept ignoring us . would go again , though . [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 72 ##48 ##8 276 4 . 0 1 4 . 0 31 . 0 1 100 ##3 . 0 100 ##4 . 333 ##33 ##33 ##33 ##33 ##34 103 ##9 . 0 100 ##1 . 0 sunday had a good time here today with my family . food was very good , wine a bit over ##pr ##ice ##d . only down ##beat was the \" mai ##tre ' d ' \" - he kept ignoring us . would go again , though . [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5824 18139 2620 25113 1018 1012 1014 1015 1018 1012 1014 2861 1012 1014 1015 2531 2509 1012 1014 2531 2549 1012 21211 22394 22394 22394 22394 22022 9800 2683 1012 1014 2531 2487 1012 1014 4465 2018 1037 2204 2051 2182 2651 2007 2026 2155 1012 2833 2001 2200 2204 1010 4511 1037 2978 2058 18098 6610 2094 1012 2069 2091 19442 2001 1996 1000 14736 7913 1005 1040 1005 1000 1011 2002 2921 9217 2149 1012 2052 2175 2153 1010 2295 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5824 18139 2620 25113 1018 1012 1014 1015 1018 1012 1014 2861 1012 1014 1015 2531 2509 1012 1014 2531 2549 1012 21211 22394 22394 22394 22394 22022 9800 2683 1012 1014 2531 2487 1012 1014 4465 2018 1037 2204 2051 2182 2651 2007 2026 2155 1012 2833 2001 2200 2204 1010 4511 1037 2978 2058 18098 6610 2094 1012 2069 2091 19442 2001 1996 1000 14736 7913 1005 1040 1005 1000 1011 2002 2921 9217 2149 1012 2052 2175 2153 1010 2295 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 91 ##60 ##5 45 ##6 5 . 0 1 5 . 0 59 . 0 1 100 ##6 . 0 100 ##3 . 5 106 ##6 . 0 100 ##1 . 0 thursday the food is excellent , we came in as a group of 10 , their crab cake is the best crab cakes i ever had , my husband ordered grill squid , oh , it is so yu ##mmy ! ! the special we ordered that night is excellent too , the service is good as well . i love this place , will def come back for more food ! [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 91 ##60 ##5 45 ##6 5 . 0 1 5 . 0 59 . 0 1 100 ##6 . 0 100 ##3 . 5 106 ##6 . 0 100 ##1 . 0 thursday the food is excellent , we came in as a group of 10 , their crab cake is the best crab cakes i ever had , my husband ordered grill squid , oh , it is so yu ##mmy ! ! the special we ordered that night is excellent too , the service is good as well . i love this place , will def come back for more food ! [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6205 16086 2629 3429 2575 1019 1012 1014 1015 1019 1012 1014 5354 1012 1014 1015 2531 2575 1012 1014 2531 2509 1012 1019 10114 2575 1012 1014 2531 2487 1012 1014 9432 1996 2833 2003 6581 1010 2057 2234 1999 2004 1037 2177 1997 2184 1010 2037 18081 9850 2003 1996 2190 18081 22619 1045 2412 2018 1010 2026 3129 3641 18651 26852 1010 2821 1010 2009 2003 2061 9805 18879 999 999 1996 2569 2057 3641 2008 2305 2003 6581 2205 1010 1996 2326 2003 2204 2004 2092 1012 1045 2293 2023 2173 1010 2097 13366 2272 2067 2005 2062 2833 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6205 16086 2629 3429 2575 1019 1012 1014 1015 1019 1012 1014 5354 1012 1014 1015 2531 2575 1012 1014 2531 2509 1012 1019 10114 2575 1012 1014 2531 2487 1012 1014 9432 1996 2833 2003 6581 1010 2057 2234 1999 2004 1037 2177 1997 2184 1010 2037 18081 9850 2003 1996 2190 18081 22619 1045 2412 2018 1010 2026 3129 3641 18651 26852 1010 2821 1010 2009 2003 2061 9805 18879 999 999 1996 2569 2057 3641 2008 2305 2003 6581 2205 1010 1996 2326 2003 2204 2004 2092 1012 1045 2293 2023 2173 1010 2097 13366 2272 2067 2005 2062 2833 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 142 ##44 ##0 77 ##9 4 . 0 1 4 . 0 1 . 0 1 100 ##3 . 0 100 ##4 . 66 ##66 ##66 ##66 ##66 ##66 ##6 103 ##1 . 66 ##66 ##66 ##66 ##66 ##66 ##7 100 ##1 . 0 saturday nice [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 142 ##44 ##0 77 ##9 4 . 0 1 4 . 0 1 . 0 1 100 ##3 . 0 100 ##4 . 66 ##66 ##66 ##66 ##66 ##66 ##6 103 ##1 . 66 ##66 ##66 ##66 ##66 ##66 ##7 100 ##1 . 0 saturday nice [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 16087 22932 2692 6255 2683 1018 1012 1014 1015 1018 1012 1014 1015 1012 1014 1015 2531 2509 1012 1014 2531 2549 1012 5764 28756 28756 28756 28756 28756 2575 9800 2487 1012 5764 28756 28756 28756 28756 28756 2581 2531 2487 1012 1014 5095 3835 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 16087 22932 2692 6255 2683 1018 1012 1014 1015 1018 1012 1014 1015 1012 1014 1015 2531 2509 1012 1014 2531 2549 1012 5764 28756 28756 28756 28756 28756 2575 9800 2487 1012 5764 28756 28756 28756 28756 28756 2581 2531 2487 1012 1014 5095 3835 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 1086 ##90 86 ##0 2 . 0 1 2 . 0 137 . 0 1 100 ##3 . 0 100 ##3 . 66 ##66 ##66 ##66 ##66 ##66 ##6 107 ##8 . 333 ##33 ##33 ##33 ##33 ##33 100 ##1 . 0 monday this was our first pizza stop in ny after we finished our statue of liberty tour . it was good but definitely not the best . first , the pizza pie ##s only come in one size forcing you to eat too much if there is only two of you or to take it home which we didn ' t want to do . there was a good variety of topping ##s ( and they had bro ##cco ##li , ya ##y ) [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 1086 ##90 86 ##0 2 . 0 1 2 . 0 137 . 0 1 100 ##3 . 0 100 ##3 . 66 ##66 ##66 ##66 ##66 ##66 ##6 107 ##8 . 333 ##33 ##33 ##33 ##33 ##33 100 ##1 . 0 monday this was our first pizza stop in ny after we finished our statue of liberty tour . it was good but definitely not the best . first , the pizza pie ##s only come in one size forcing you to eat too much if there is only two of you or to take it home which we didn ' t want to do . there was a good variety of topping ##s ( and they had bro ##cco ##li , ya ##y ) [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 28196 21057 6564 2692 1016 1012 1014 1015 1016 1012 1014 14989 1012 1014 1015 2531 2509 1012 1014 2531 2509 1012 5764 28756 28756 28756 28756 28756 2575 10550 2620 1012 21211 22394 22394 22394 22394 22394 2531 2487 1012 1014 6928 2023 2001 2256 2034 10733 2644 1999 6396 2044 2057 2736 2256 6231 1997 7044 2778 1012 2009 2001 2204 2021 5791 2025 1996 2190 1012 2034 1010 1996 10733 11345 2015 2069 2272 1999 2028 2946 6932 2017 2000 4521 2205 2172 2065 2045 2003 2069 2048 1997 2017 2030 2000 2202 2009 2188 2029 2057 2134 1005 1056 2215 2000 2079 1012 2045 2001 1037 2204 3528 1997 22286 2015 1006 1998 2027 2018 22953 21408 3669 1010 8038 2100 1007 102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 28196 21057 6564 2692 1016 1012 1014 1015 1016 1012 1014 14989 1012 1014 1015 2531 2509 1012 1014 2531 2509 1012 5764 28756 28756 28756 28756 28756 2575 10550 2620 1012 21211 22394 22394 22394 22394 22394 2531 2487 1012 1014 6928 2023 2001 2256 2034 10733 2644 1999 6396 2044 2057 2736 2256 6231 1997 7044 2778 1012 2009 2001 2204 2021 5791 2025 1996 2190 1012 2034 1010 1996 10733 11345 2015 2069 2272 1999 2028 2946 6932 2017 2000 4521 2205 2172 2065 2045 2003 2069 2048 1997 2017 2030 2000 2202 2009 2188 2029 2057 2134 1005 1056 2215 2000 2079 1012 2045 2001 1037 2204 3528 1997 22286 2015 1006 1998 2027 2018 22953 21408 3669 1010 8038 2100 1007 102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 55 ##26 ##9 65 ##5 1 . 0 1 1 . 0 175 . 0 1 100 ##5 . 0 100 ##3 . 0 113 ##7 . 6 100 ##1 . 0 monday * * * * * be ##ware of the host and waits ##taff * * * * * my date and i came here early on a saturday because we had heard great things . upon walking in , we were immediately acc ##ost ##ed by the host inter ##ro ##gating us on how long we would be . after sitting down and ordering drinks , we were constantly hound ##ed to order app ##eti ##zers and food . after finishing the meal and the last of our over ##pr ##ice ##d wine [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 55 ##26 ##9 65 ##5 1 . 0 1 1 . 0 175 . 0 1 100 ##5 . 0 100 ##3 . 0 113 ##7 . 6 100 ##1 . 0 monday * * * * * be ##ware of the host and waits ##taff * * * * * my date and i came here early on a saturday because we had heard great things . upon walking in , we were immediately acc ##ost ##ed by the host inter ##ro ##gating us on how long we would be . after sitting down and ordering drinks , we were constantly hound ##ed to order app ##eti ##zers and food . after finishing the meal and the last of our over ##pr ##ice ##d wine [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4583 23833 2683 3515 2629 1015 1012 1014 1015 1015 1012 1014 12862 1012 1014 1015 2531 2629 1012 1014 2531 2509 1012 1014 12104 2581 1012 1020 2531 2487 1012 1014 6928 1008 1008 1008 1008 1008 2022 8059 1997 1996 3677 1998 18074 22542 1008 1008 1008 1008 1008 2026 3058 1998 1045 2234 2182 2220 2006 1037 5095 2138 2057 2018 2657 2307 2477 1012 2588 3788 1999 1010 2057 2020 3202 16222 14122 2098 2011 1996 3677 6970 3217 16961 2149 2006 2129 2146 2057 2052 2022 1012 2044 3564 2091 1998 13063 8974 1010 2057 2020 7887 19598 2098 2000 2344 10439 20624 16750 1998 2833 1012 2044 5131 1996 7954 1998 1996 2197 1997 2256 2058 18098 6610 2094 4511 102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4583 23833 2683 3515 2629 1015 1012 1014 1015 1015 1012 1014 12862 1012 1014 1015 2531 2629 1012 1014 2531 2509 1012 1014 12104 2581 1012 1020 2531 2487 1012 1014 6928 1008 1008 1008 1008 1008 2022 8059 1997 1996 3677 1998 18074 22542 1008 1008 1008 1008 1008 2026 3058 1998 1045 2234 2182 2220 2006 1037 5095 2138 2057 2018 2657 2307 2477 1012 2588 3788 1999 1010 2057 2020 3202 16222 14122 2098 2011 1996 3677 6970 3217 16961 2149 2006 2129 2146 2057 2052 2022 1012 2044 3564 2091 1998 13063 8974 1010 2057 2020 7887 19598 2098 2000 2344 10439 20624 16750 1998 2833 1012 2044 5131 1996 7954 1998 1996 2197 1997 2256 2058 18098 6610 2094 4511 102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 10000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 123 ##9 ##25 77 4 . 0 1 4 . 0 20 . 0 1 101 ##9 . 0 100 ##3 . 94 ##7 ##36 ##8 ##42 ##10 ##52 ##6 109 ##1 . 68 ##42 ##10 ##52 ##6 ##31 ##58 100 ##1 . 0 friday great food & friendly service ! make sure you try the market sides . . strawberry cobb ##ler and the layer chocolate ( when they have it ) [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 123 ##9 ##25 77 4 . 0 1 4 . 0 20 . 0 1 101 ##9 . 0 100 ##3 . 94 ##7 ##36 ##8 ##42 ##10 ##52 ##6 109 ##1 . 68 ##42 ##10 ##52 ##6 ##31 ##58 100 ##1 . 0 friday great food & friendly service ! make sure you try the market sides . . strawberry cobb ##ler and the layer chocolate ( when they have it ) [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 13138 2683 17788 6255 1018 1012 1014 1015 1018 1012 1014 2322 1012 1014 1015 7886 2683 1012 1014 2531 2509 1012 6365 2581 21619 2620 20958 10790 25746 2575 11518 2487 1012 6273 20958 10790 25746 2575 21486 27814 2531 2487 1012 1014 5958 2307 2833 1004 5379 2326 999 2191 2469 2017 3046 1996 3006 3903 1012 1012 16876 17176 3917 1998 1996 6741 7967 1006 2043 2027 2031 2009 1007 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 13138 2683 17788 6255 1018 1012 1014 1015 1018 1012 1014 2322 1012 1014 1015 7886 2683 1012 1014 2531 2509 1012 6365 2581 21619 2620 20958 10790 25746 2575 11518 2487 1012 6273 20958 10790 25746 2575 21486 27814 2531 2487 1012 1014 5958 2307 2833 1004 5379 2326 999 2191 2469 2017 3046 1996 3006 3903 1012 1012 16876 17176 3917 1998 1996 6741 7967 1006 2043 2027 2031 2009 1007 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 49 ##25 13 5 . 0 2 5 . 0 38 . 0 2 100 ##8 . 0 100 ##4 . 0 108 ##7 . 75 100 ##1 . 0 thursday some of the best seasoned rot ##isse ##rie chicken you can find in the area ! it has the feeling of being back in coa ##mo , pr . love it . [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 49 ##25 13 5 . 0 2 5 . 0 38 . 0 2 100 ##8 . 0 100 ##4 . 0 108 ##7 . 75 100 ##1 . 0 thursday some of the best seasoned rot ##isse ##rie chicken you can find in the area ! it has the feeling of being back in coa ##mo , pr . love it . [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4749 17788 2410 1019 1012 1014 1016 1019 1012 1014 4229 1012 1014 1016 2531 2620 1012 1014 2531 2549 1012 1014 10715 2581 1012 4293 2531 2487 1012 1014 9432 2070 1997 1996 2190 28223 18672 23491 7373 7975 2017 2064 2424 1999 1996 2181 999 2009 2038 1996 3110 1997 2108 2067 1999 28155 5302 1010 10975 1012 2293 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4749 17788 2410 1019 1012 1014 1016 1019 1012 1014 4229 1012 1014 1016 2531 2620 1012 1014 2531 2549 1012 1014 10715 2581 1012 4293 2531 2487 1012 1014 9432 2070 1997 1996 2190 28223 18672 23491 7373 7975 2017 2064 2424 1999 1996 2181 999 2009 2038 1996 3110 1997 2108 2067 1999 28155 5302 1010 10975 1012 2293 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 56 ##38 ##7 243 3 . 0 2 3 . 5 11 . 0 1 104 ##4 . 0 100 ##3 . 63 ##6 ##36 ##36 ##36 ##36 ##36 112 ##0 . 25 100 ##2 . 0 thursday pizza was ok , but over priced . [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 56 ##38 ##7 243 3 . 0 2 3 . 5 11 . 0 1 104 ##4 . 0 100 ##3 . 63 ##6 ##36 ##36 ##36 ##36 ##36 112 ##0 . 25 100 ##2 . 0 thursday pizza was ok , but over priced . [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5179 22025 2581 22884 1017 1012 1014 1016 1017 1012 1019 2340 1012 1014 1015 9645 2549 1012 1014 2531 2509 1012 6191 2575 21619 21619 21619 21619 21619 11176 2692 1012 2423 2531 2475 1012 1014 9432 10733 2001 7929 1010 2021 2058 21125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 5179 22025 2581 22884 1017 1012 1014 1016 1017 1012 1019 2340 1012 1014 1015 9645 2549 1012 1014 2531 2509 1012 6191 2575 21619 21619 21619 21619 21619 11176 2692 1012 2423 2531 2475 1012 1014 9432 10733 2001 7929 1010 2021 2058 21125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 60 ##70 ##0 52 ##7 4 . 0 1 4 . 0 68 . 0 1 101 ##2 . 0 100 ##4 . 41 ##66 ##66 ##66 ##66 ##66 ##6 105 ##8 . 91 ##66 ##66 ##66 ##66 ##66 ##7 100 ##1 . 0 wednesday an affordable organic vega ##n place . i like the sandwiches i have tried so far . my colleague refuses to go back here after trying their salad though . i liked their free samples when they had just opened . however , i feel the drinks are ridiculous ##ly expensive . the coconut water i was tempted to buy a couple of times was $ 8 : - / but i keep going there for food . good service . [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 60 ##70 ##0 52 ##7 4 . 0 1 4 . 0 68 . 0 1 101 ##2 . 0 100 ##4 . 41 ##66 ##66 ##66 ##66 ##66 ##6 105 ##8 . 91 ##66 ##66 ##66 ##66 ##66 ##7 100 ##1 . 0 wednesday an affordable organic vega ##n place . i like the sandwiches i have tried so far . my colleague refuses to go back here after trying their salad though . i liked their free samples when they had just opened . however , i feel the drinks are ridiculous ##ly expensive . the coconut water i was tempted to buy a couple of times was $ 8 : - / but i keep going there for food . good service . [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3438 19841 2692 4720 2581 1018 1012 1014 1015 1018 1012 1014 6273 1012 1014 1015 7886 2475 1012 1014 2531 2549 1012 4601 28756 28756 28756 28756 28756 2575 8746 2620 1012 6205 28756 28756 28756 28756 28756 2581 2531 2487 1012 1014 9317 2019 15184 7554 15942 2078 2173 1012 1045 2066 1996 22094 1045 2031 2699 2061 2521 1012 2026 11729 10220 2000 2175 2067 2182 2044 2667 2037 16521 2295 1012 1045 4669 2037 2489 8168 2043 2027 2018 2074 2441 1012 2174 1010 1045 2514 1996 8974 2024 9951 2135 6450 1012 1996 16027 2300 1045 2001 16312 2000 4965 1037 3232 1997 2335 2001 1002 1022 1024 1011 1013 2021 1045 2562 2183 2045 2005 2833 1012 2204 2326 1012 102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3438 19841 2692 4720 2581 1018 1012 1014 1015 1018 1012 1014 6273 1012 1014 1015 7886 2475 1012 1014 2531 2549 1012 4601 28756 28756 28756 28756 28756 2575 8746 2620 1012 6205 28756 28756 28756 28756 28756 2581 2531 2487 1012 1014 9317 2019 15184 7554 15942 2078 2173 1012 1045 2066 1996 22094 1045 2031 2699 2061 2521 1012 2026 11729 10220 2000 2175 2067 2182 2044 2667 2037 16521 2295 1012 1045 4669 2037 2489 8168 2043 2027 2018 2074 2441 1012 2174 1010 1045 2514 1996 8974 2024 9951 2135 6450 1012 1996 16027 2300 1045 2001 16312 2000 4965 1037 3232 1997 2335 2001 1002 1022 1024 1011 1013 2021 1045 2562 2183 2045 2005 2833 1012 2204 2326 1012 102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 253 ##6 ##7 75 ##9 4 . 0 1 4 . 0 99 . 0 1 105 ##9 . 0 100 ##4 . 1016 ##9 ##49 ##15 ##25 ##43 110 ##7 . 08 ##47 ##45 ##7 ##6 ##27 ##12 100 ##2 . 0 thursday went to il ##ili for summer restaurant week recently and was happy with the all round experience . our waiter was fairly new , but was great with his recommendations and an absolute pleasure - had a perfect balance of humor and professional ##ism . a lot of the dishes we had - chicken liver , lamb shoulder - were very rich but balanced out well with the yo ##gh ##urt sauce ##s etc . , and lighter dishes li ##k the [SEP]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] 253 ##6 ##7 75 ##9 4 . 0 1 4 . 0 99 . 0 1 105 ##9 . 0 100 ##4 . 1016 ##9 ##49 ##15 ##25 ##43 110 ##7 . 08 ##47 ##45 ##7 ##6 ##27 ##12 100 ##2 . 0 thursday went to il ##ili for summer restaurant week recently and was happy with the all round experience . our waiter was fairly new , but was great with his recommendations and an absolute pleasure - had a perfect balance of humor and professional ##ism . a lot of the dishes we had - chicken liver , lamb shoulder - were very rich but balanced out well with the yo ##gh ##urt sauce ##s etc . , and lighter dishes li ##k the [SEP]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 23254 2575 2581 4293 2683 1018 1012 1014 1015 1018 1012 1014 5585 1012 1014 1015 8746 2683 1012 1014 2531 2549 1012 28707 2683 26224 16068 17788 23777 7287 2581 1012 5511 22610 19961 2581 2575 22907 12521 2531 2475 1012 1014 9432 2253 2000 6335 18622 2005 2621 4825 2733 3728 1998 2001 3407 2007 1996 2035 2461 3325 1012 2256 15610 2001 7199 2047 1010 2021 2001 2307 2007 2010 11433 1998 2019 7619 5165 1011 2018 1037 3819 5703 1997 8562 1998 2658 2964 1012 1037 2843 1997 1996 10447 2057 2018 1011 7975 11290 1010 12559 3244 1011 2020 2200 4138 2021 12042 2041 2092 2007 1996 10930 5603 19585 12901 2015 4385 1012 1010 1998 9442 10447 5622 2243 1996 102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 23254 2575 2581 4293 2683 1018 1012 1014 1015 1018 1012 1014 5585 1012 1014 1015 8746 2683 1012 1014 2531 2549 1012 28707 2683 26224 16068 17788 23777 7287 2581 1012 5511 22610 19961 2581 2575 22907 12521 2531 2475 1012 1014 9432 2253 2000 6335 18622 2005 2621 4825 2733 3728 1998 2001 3407 2007 1996 2035 2461 3325 1012 2256 15610 2001 7199 2047 1010 2021 2001 2307 2007 2010 11433 1998 2019 7619 5165 1011 2018 1037 3819 5703 1997 8562 1998 2658 2964 1012 1037 2843 1997 1996 10447 2057 2018 1011 7975 11290 1010 12559 3244 1011 2020 2200 4138 2021 12042 2041 2092 2007 1996 10930 5603 19585 12901 2015 4385 1012 1010 1998 9442 10447 5622 2243 1996 102\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq"
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "  # Use \"sequence_outputs\" for token-level output.\n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.55)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting:\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpE0ZIDOCQzE"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for train, evaluation, and prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnH-AnOQ9KKW"
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjwJ4bTeWXD8"
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 8.0\n",
        "# Warmup is a period of time where the learning rate \n",
        "# is small and gradually increases--usually helps train.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 100\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emHf9GhfWBZ_"
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_d9vU3KgIOt",
        "outputId": "0fe01e71-b2a8-4b51-81ff-83444f24c820"
      },
      "source": [
        "num_train_steps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2781"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJldMr3WYZa"
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_WebpS1X97v",
        "outputId": "5dc916a5-f097-43d2-efab-ee407283b820"
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR_NAME5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f94da91bc50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR_NAME5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f94da91bc50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOO3RfG1DYLo"
      },
      "source": [
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pv2bAlOX_-K"
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Nukby2EB6-"
      },
      "source": [
        "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nucD4gluYJmK",
        "outputId": "f724f554-d84d-460f-8ec6-5350b8fe35ef"
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-24-13129b192727>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-24-13129b192727>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.8470311, step = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.8470311, step = 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 100 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 100 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.0256505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.0256505\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 100 vs previous value: 100. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 100 vs previous value: 100. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.67839, step = 101 (3898.575 sec)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.67839, step = 101 (3898.575 sec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 113 vs previous value: 113. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 113 vs previous value: 113. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 117 vs previous value: 117. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 117 vs previous value: 117. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 124 vs previous value: 124. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 124 vs previous value: 124. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 159 vs previous value: 159. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 159 vs previous value: 159. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 200 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 200 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.0259404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.0259404\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.41754425, step = 201 (3854.984 sec)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.41754425, step = 201 (3854.984 sec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 300 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 300 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.025985\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.025985\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6209652, step = 301 (3848.375 sec)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6209652, step = 301 (3848.375 sec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 400 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 400 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.0260248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.0260248\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.32645023, step = 401 (3842.476 sec)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.32645023, step = 401 (3842.476 sec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 500 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 500 into OUTPUT_DIR_NAME5/model.ckpt.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.0259275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.0259275\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.18896419, step = 501 (3856.907 sec)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.18896419, step = 501 (3856.907 sec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbLTVniARy3"
      },
      "source": [
        "Now let's use our test data to see how well our model did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIhejfpyJ8Bx"
      },
      "source": [
        "valid_input_fn = run_classifier.input_fn_builder(\n",
        "    features=valid_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPVEXhNjYXC-"
      },
      "source": [
        "estimator.evaluate(input_fn=valid_input_fn, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9JMKDENl_Z9"
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP24-avAmGAP"
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPVLlORiy1X-"
      },
      "source": [
        "predictions = estimator.predict(test_input_fn)\n",
        "#list(predictions)[0]\n",
        "pred = pd.DataFrame(list(predictions))\n",
        "pred.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgnRFWdiy44I"
      },
      "source": [
        "import numpy as np\n",
        "pred[['log_p0', 'log_p1']] = pd.DataFrame(pred.probabilities.tolist()) \n",
        "pred['p1']= np.exp(pred['log_p1'])\n",
        "\n",
        "pred.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJr0KG7zKKi"
      },
      "source": [
        "test = (test).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOVzhB3BzRDp"
      },
      "source": [
        "(test.label == pred.labels).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGuVJaIkzYvR"
      },
      "source": [
        "pred['true_labels'] = test.label\n",
        "pred['abs_dif'] = np.abs(pred.true_labels-pred.p1)\n",
        "pred['review'] = test['review']\n",
        "pred.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-xgOPKqzlYo"
      },
      "source": [
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(pred.true_labels , pred.p1)\n",
        "auc = metrics.roc_auc_score(pred.true_labels , pred.p1)\n",
        "print(\"Auc ROC is \",auc)\n",
        "precision, recall, thresholds = metrics.precision_recall_curve(pred.true_labels , pred.p1)\n",
        "auc = metrics.auc(recall, precision)\n",
        "print(\"Auc PRC is \",auc)\n",
        "f1 = metrics.f1_score(pred.true_labels, pred.labels)\n",
        "print(\"F1 score is\",f1)\n",
        "print(\"Log loss is \",metrics.log_loss(pred.true_labels , pred.p1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdRXWjbE1yan"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plot the roc curve for the model\n",
        "plt.plot(fpr, tpr, linestyle='--', label='BERT')\n",
        "#plt.plot(fpr_ll, tpr_ll, linestyle='-', label='Logistic Regression')\n",
        "#pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "plt.legend()\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRAzDnMTihQ1"
      },
      "source": [
        "results = pd.DataFrame()\n",
        "results['prob'] = pred.p1\n",
        "results['predictions'] = pred.labels\n",
        "results['labels'] = pred.true_labels\n",
        "results.to_csv('/content/gdrive/My Drive/Colab Notebooks/BERT.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzVaxo0jQ9ii"
      },
      "source": [
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1OqpNncRHjM"
      },
      "source": [
        "estimator.evaluate(input_fn=train_input_fn, steps=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS1f-_eZJhko"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}